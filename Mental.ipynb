{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tripathi-Krishna17/Mental-Health-Analysis/blob/main/Mental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==========================================\n",
        "# 1. SETUP & INSTALLATION\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "RhS_lSTO4hg2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qjl_gqEynDw"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install datasets transformers xgboost scikit-learn accelerate torch -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==========================================\n",
        "# 2. DATA LOADING & PREPROCESSING\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "4hiXQHi84caz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Loading dataset...\")\n",
        "# Loading the dataset from Hugging Face\n",
        "dataset = load_dataset(\"solomonk/reddit_mental_health_posts\")\n",
        "\n",
        "# Convert to Pandas DataFrame for easier manipulation\n",
        "df = dataset['train'].to_pandas()"
      ],
      "metadata": {
        "id": "MzPneirfy02M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "E2gM6BQ70AN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Cleaning ---\n",
        "print(\"Cleaning data...\")\n",
        "\n",
        "# 1. Filter necessary columns (assuming structure based on reddit datasets)\n",
        "# Inspecting columns to ensure we pick the right ones\n",
        "print(f\"Available columns: {df.columns}\")\n",
        "# Usually 'title', 'body', and 'subreddit' are key.\n",
        "# We will fill NaNs and combine title + body for richer context.\n",
        "df['title'] = df['title'].fillna('')\n",
        "df['body'] = df['body'].fillna('')\n",
        "df['text'] = df['title'] + \" \" + df['body']"
      ],
      "metadata": {
        "id": "KXHeq8Z50SxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Filter for top classes to ensure model stability (Multi-class classification)\n",
        "# Let's focus on the top 5 most frequent subreddits to make the demo robust and runnable\n",
        "top_n_classes = 5\n",
        "top_subreddits = df['subreddit'].value_counts().nlargest(top_n_classes).index\n",
        "df = df[df['subreddit'].isin(top_subreddits)]"
      ],
      "metadata": {
        "id": "g3JmLvJs0mrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Sampling (Optional but recommended for Colab speed)\n",
        "# Using 2000 samples for demonstration. Comment this out to use the full dataset.\n",
        "df = df.sample(n=2000, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "0bKXeQyl2D3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = text.lower() # Lowercase\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text) # Remove mentions/hashtags\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove special chars/numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "-HPtFd9Z2RB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "qFLbY_oL3IAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Label Encoding\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['subreddit'])\n",
        "num_labels = len(le.classes_)\n",
        "print(f\"Classes to predict: {le.classes_}\")"
      ],
      "metadata": {
        "id": "aKRmMiVd4mxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    df['clean_text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
        ")"
      ],
      "metadata": {
        "id": "lxynLYb84nkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==========================================\n",
        "# 3. FEATURE ENGINEERING (Traditional Models)\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "SMzfegm94MqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Feature Engineering for Traditional Models...\")\n",
        "# TF-IDF Vectorization for XGBoost and SVM\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text).toarray()\n",
        "X_test_tfidf = tfidf.transform(X_test_text).toarray()"
      ],
      "metadata": {
        "id": "XzsXvU4Z438y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf"
      ],
      "metadata": {
        "id": "eLW5qvhG5soy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==========================================\n",
        "# 4. MODEL 1: BERT (Deep Learning)\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "N1cl1hWP4DLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Model 1: BERT...\")\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "BMfNSAkn57rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare datasets\n",
        "train_dataset = RedditDataset(X_train_text.tolist(), y_train.tolist(), tokenizer)\n",
        "test_dataset = RedditDataset(X_test_text.tolist(), y_test.tolist(), tokenizer)"
      ],
      "metadata": {
        "id": "zbwrXAbC1ozz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "model_bert = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=num_labels\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "F2rPxMIA2D0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "Jwr7MFtf2G-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model_bert,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "hGcqEMVu2Mus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "# Get BERT Predictions (Probabilities)\n",
        "print(\"Generating BERT predictions...\")\n",
        "bert_preds_output = trainer.predict(test_dataset)\n",
        "prob_bert = torch.nn.functional.softmax(torch.tensor(bert_preds_output.predictions), dim=-1).numpy()"
      ],
      "metadata": {
        "id": "4sCLclrT2pWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==========================================\n",
        "# 5. MODEL 2: XGBoost\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "nv5xMx3u364-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Model 2: XGBoost...\")\n",
        "# XGBoost requires numerical input (provided by TF-IDF)\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")"
      ],
      "metadata": {
        "id": "TL7eAxf_3_E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "65UIdudO4sXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get XGBoost Predictions (Probabilities)\n",
        "prob_xgb = xgb_model.predict_proba(X_test_tfidf)"
      ],
      "metadata": {
        "id": "0fpiJ2s34vm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==========================================\n",
        "# 6. MODEL 3: SVM\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "nsSDb4Z944-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Model 3: SVM...\")\n",
        "# SVM needs probability=True to output class probabilities for the ensemble\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "svm_model.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "u8MtR8LV45yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get SVM Predictions (Probabilities)\n",
        "prob_svm = svm_model.predict_proba(X_test_tfidf)"
      ],
      "metadata": {
        "id": "t9oomKBf4-Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==========================================\n",
        "# 7. ENSEMBLE & EVALUATION\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "3uooK4r75vAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Combining models (Ensemble)...\")\n",
        "\n",
        "# Soft Voting Ensemble: Average the probabilities from all 3 models\n",
        "# You can assign weights if one model is significantly better (e.g., 0.5*BERT + 0.25*XGB + 0.25*SVM)\n",
        "avg_preds = (prob_bert + prob_xgb + prob_svm) / 3\n",
        "final_predictions = np.argmax(avg_preds, axis=1)"
      ],
      "metadata": {
        "id": "GMYwlxyQ5vW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average Prediction:\\n \",avg_preds, \"\\nFinal predictions:\\n\", final_predictions)"
      ],
      "metadata": {
        "id": "iqVTmV435zRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "print(\"\\n=== Final Ensemble Classification Report ===\")\n",
        "print(classification_report(y_test, final_predictions, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "uGJkOORw6ESp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"BERT Accuracy: {accuracy_score(y_test, np.argmax(prob_bert, axis=1)):.4f}\")\n",
        "print(f\"XGBoost Accuracy: {accuracy_score(y_test, np.argmax(prob_xgb, axis=1)):.4f}\")\n",
        "print(f\"SVM Accuracy: {accuracy_score(y_test, np.argmax(prob_svm, axis=1)):.4f}\")\n",
        "print(f\"Ensemble Accuracy: {accuracy_score(y_test, final_predictions):.4f}\")"
      ],
      "metadata": {
        "id": "jTgvt9x-6Vdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3sYpIvwt6b0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}